<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AI Voice Agent - Streaming</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <div class="wrapper">
    <h1>AI Voice Agent - Streaming Audio</h1>
    <div class="mic-btn" id="record-btn">
      <span class="mic-icon">üéôÔ∏è</span>
    </div>
    <p id="chat-status">Press the button to start streaming</p>
    <div id="transcription-output" style="margin-top: 20px; font-size: 18px; color: #333;"></div>
  </div>
  <script>
    let ws, audioContext, source, processor;
    const recordBtn = document.getElementById("record-btn");
    const chatStatus = document.getElementById("chat-status");
    const transcriptionOutput = document.getElementById("transcription-output");

    async function startRecording() {
      try {
        ws = new WebSocket("ws://localhost:8000/ws/audio");
        ws.onopen = async () => {
          chatStatus.textContent = "Streaming started...";
          console.log("Connected to server");
          transcriptionOutput.textContent = ""; // Clear previous transcription

          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
          source = audioContext.createMediaStreamSource(stream);
          processor = audioContext.createScriptProcessor(4096, 1, 1); // Mono, 4096 buffer size

          processor.onaudioprocess = (event) => {
            const inputData = event.inputBuffer.getChannelData(0);
            const pcmData = new Int16Array(inputData.length);
            for (let i = 0; i < inputData.length; i++) {
              const sample = Math.max(-1, Math.min(1, inputData[i]));
              pcmData[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            }
            if (ws.readyState === WebSocket.OPEN) {
              console.log(Sending PCM chunk of size: ${pcmData.buffer.byteLength} bytes);
              ws.send(pcmData.buffer);
            }
          };

          source.connect(processor);
          processor.connect(audioContext.destination);

          // Handle incoming messages (transcriptions)
          ws.onmessage = (event) => {
            const transcript = event.data;
            if (transcript) {
              transcriptionOutput.textContent += transcript + " ";
              transcriptionOutput.scrollTop = transcriptionOutput.scrollHeight; // Auto-scroll
            }
          };
        };

        ws.onclose = () => {
          chatStatus.textContent = "Streaming stopped.";
          console.log("WebSocket closed");
        };
      } catch (err) {
        console.error("Microphone error:", err);
        chatStatus.textContent = "Microphone access denied.";
      }
    }

    function stopRecording() {
      if (processor) {
        processor.disconnect();
        source.disconnect();
        audioContext.close();
        chatStatus.textContent = "Stopping recording...";
      }
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send("EOF");
        ws.close();
      }
    }

    recordBtn.addEventListener("click", () => {
      recordBtn.classList.add("clicked");
      setTimeout(() => recordBtn.classList.remove("clicked"), 200);
      if (!recordBtn.classList.contains("recording")) {
        startRecording();
        recordBtn.classList.add("recording");
      } else {
        stopRecording();
        recordBtn.classList.remove("recording");
      }
    });
  </script>
</body>
</html>
