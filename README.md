

# 30 Days of AI Voice Challenge â€“ Murf AI

An interactive journey of building **AI-powered Voice Agents** in 30 days, integrating cutting-edge APIs for **speech-to-text, natural language processing, and text-to-speech**.

This project highlights daily progress, showcasing how AI voice technology can be combined with real-time APIs to create powerful conversational systems.

---

##  What This Project Covers

* **Murf API** â€“ Natural voice Text-to-Speech conversion
* **AssemblyAI API** â€“ Speech-to-Text transcription
* **Google Gemini API** â€“ Natural Language Understanding & Response Generation
* **FastAPI Backend** â€“ Real-time API endpoints
* **Interactive Frontend** â€“ Record, transcribe, and play AI-generated responses

---

##  Features

*  **Speech Recognition** â€“ Convert spoken input to text (AssemblyAI)
*  **Intelligent Response** â€“ AI-driven replies with Google Gemini
*  **Natural Voice Output** â€“ Generate lifelike speech with Murf AI
*  **Real-Time Processing** â€“ Powered by FastAPI and WebSockets
*  **Browser Interface** â€“ Simple HTML/CSS/JS frontend

---

##  Tech Stack

* **Frontend** â€“ HTML, CSS, JavaScript
* **Backend** â€“ Python (FastAPI)
* **APIs** â€“

  * [Murf AI](https://murf.ai) â€“ Text-to-Speech
  * [AssemblyAI](https://www.assemblyai.com) â€“ Speech-to-Text
  * [Google Gemini](https://ai.google) â€“ AI for NLP
* **Others** â€“ WebSockets, Fetch API, dotenv for config

---

##  Project Structure

```
voice-agent/
â”‚â”€â”€ .venv/                       # Virtual environment 
â”‚â”€â”€ audio_outputs/               # Stores generated AI voice audio files
â”‚â”€â”€ static/                      # Frontend static files
â”‚   â”œâ”€â”€ index.html               # Main user interface
â”‚   â”œâ”€â”€ index.js                 # Handles frontend logic & API calls
â”‚   â”œâ”€â”€ style.css                # UI styling
â”‚   â””â”€â”€ screenshots/             # Daily progress screenshots
â”‚â”€â”€ uploads/                     # Stores uploaded audio files
â”‚â”€â”€ .env                         # Environment variables (API keys)
â”‚â”€â”€ main.py                      # FastAPI backend
â”‚â”€â”€ README.md                    # Documentation
â”‚â”€â”€ requirements.txt             # Python dependencies
```

---

##  Setup & Installation

###  Clone Repository

```bash
git clone https://github.com/chaitravc/DAYS-30.git
cd DAYS-30
```

###  Install Dependencies

```bash
pip install -r requirements.txt
```

### Add Environment Variables

Create a `.env` file in the **root** directory:

```env
MURF_API_KEY=your_murf_api_key_here
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
GOOGLE_API_KEY=your_gemini_api_key_here
```

###  Run Backend

```bash
uvicorn main:app --reload
```

Visit â†’ [http://127.0.0.1:8000](http://127.0.0.1:8000)

### Open Frontend

In browser â†’

```
http://127.0.0.1:8000/static/index.html
```

---

##  Workflow

1. User clicks **Record** and speaks
2. Audio â†’ **AssemblyAI** (Speech-to-Text)
3. Text â†’ **Google Gemini** (AI Response)
4. Reply â†’ **Murf AI** (Voice Output)
5. Response is played back in the browser

---

## ðŸ“¡ API Endpoint

| Method | Endpoint          | Description                         |
| ------ | ----------------- | ----------------------------------- |
| POST   | `/api/agent/chat` | Send audio & receive AI voice reply |

---

## ðŸ“¦ Dependencies

* `fastapi`
* `uvicorn`
* `requests`
* `python-dotenv`
* `pydantic`

Install manually:

```bash
pip install fastapi uvicorn requests python-dotenv pydantic
```

---

##  30 Days Challenge

This repo documents my **daily progress** of building an AI Voice Agent:


âœ” Day 20 â€“ WebSocket-based streaming


---

##  Goal

To build and showcase an **end-to-end AI-powered Voice Agent** that can:

* Listen to speech
* Understand context
* Respond intelligently
* Speak back naturally

---



---

ðŸ‘‰ Perfect for your **GitHub repo** documenting your 30 Days AI Voice Challenge.

Do you also want me to make a **shorter LinkedIn-style README summary** (like a project highlight) that you can share as a post?
